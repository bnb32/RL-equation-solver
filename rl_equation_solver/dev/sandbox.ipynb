{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rl_equation_solver\n",
    "from rl_equation_solver.environment.algebraic import Env\n",
    "from rl_equation_solver.agent.dqn import Agent as AgentDQN\n",
    "from rl_equation_solver.agent.gcn import Agent as AgentGCN\n",
    "from rl_equation_solver.agent.lstm import Agent as AgentLSTM\n",
    "from rl_equation_solver.utilities import utilities\n",
    "from rl_equation_solver.utilities.utilities import GraphEmbedding\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "from rex import init_logger\n",
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gymnasium import spaces\n",
    "from gymnasium.vector.utils.spaces import batch_space\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from sympy import symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logger(__name__, log_level='DEBUG')\n",
    "init_logger('rl_equation_solver', log_level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_env(env)\n",
    "model.learn(total_timesteps=int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset_history()\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hist_plot(env, start=0):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "    avg_complex = []\n",
    "    avg_reward = []\n",
    "    avg_loss = []\n",
    "    for episode in list(env.history.keys())[start:]:\n",
    "        avg_complex.append(np.mean(env.history[episode]['complexity']))\n",
    "        avg_loss.append(np.nanmean(env.history[episode]['loss']))\n",
    "        avg_reward.append(np.mean(env.history[episode]['reward']))\n",
    "\n",
    "    plt.hist(avg_reward)\n",
    "    plt.xlabel('Reward')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "def moving_avg(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "def make_plot(env, start=0):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 5))\n",
    "    avg_complex = []\n",
    "    avg_reward = []\n",
    "    avg_loss = []\n",
    "    for episode in list(env.history.keys())[start:]:\n",
    "        avg_complex.append(np.mean(env.history[episode]['complexity']))\n",
    "        avg_loss.append(np.nanmean(env.history[episode]['loss']))\n",
    "        avg_reward.append(np.mean(env.history[episode]['reward']))\n",
    "    \n",
    "    y = moving_avg(avg_complex, 1)\n",
    "    x = np.arange(len(y))\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    ax[0].scatter(x, y)\n",
    "    ax[0].plot(x, a*x+b, color='red')\n",
    "\n",
    "    y = moving_avg(avg_loss, 1)\n",
    "    x = np.arange(len(y))\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    ax[1].scatter(x, y)\n",
    "    ax[1].plot(x, a*x+b, color='red')\n",
    "    \n",
    "    y = moving_avg(avg_reward, 1)\n",
    "    x = np.arange(len(y))\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    ax[2].scatter(x, y)\n",
    "    ax[2].plot(x, a*x+b, color='red')\n",
    "    \n",
    "    ax[0].set_title('Complexity')\n",
    "    ax[1].set_title('Loss')\n",
    "    ax[2].set_title(\"Reward\")\n",
    "    plt.annotate('Episode', (0.4, 0.01), xycoords='figure fraction')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_hist_plot(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_solver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
